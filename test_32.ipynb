{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "['CPLEX', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'OSQP', 'SCIPY', 'SCS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import expm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "scaler = MinMaxScaler()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "device1 = torch.device('cpu')\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print(cp.installed_solvers())\n",
    "\n",
    "\n",
    "def error_point(prep, real):\n",
    "    prep_0 = prep\n",
    "    error_point = np.array([], dtype=int)\n",
    "    prep_full_label = np.setdiff1d(np.unique(prep), np.array([-1]))\n",
    "    real_full_label = np.setdiff1d(np.unique(real), np.array([-1]))\n",
    "    nonnoise_index = np.intersect1d(\n",
    "        np.where(prep != -1)[0],\n",
    "        np.where(real != -1)[0])\n",
    "    real = real[nonnoise_index]\n",
    "    prep = prep[nonnoise_index]\n",
    "    real_label = np.unique(real)\n",
    "    prep_label = np.unique(prep)\n",
    "    n = len(real_label)\n",
    "    n_1 = len(prep_label)\n",
    "    reallogic = (np.reshape(np.repeat(real, n), [len(real), n])\n",
    "                 == real_label).T + 0\n",
    "    preplogic = (np.reshape(np.repeat(prep, n_1), [len(prep), n_1])\n",
    "                 == prep_label).T + 0\n",
    "    interset_matrix = reallogic @ preplogic.T\n",
    "    x = cp.Variable((n, n_1), integer=True)\n",
    "    obj = cp.Minimize(-cp.sum(cp.multiply(interset_matrix, x)))\n",
    "    con = [\n",
    "        0 <= x, x <= 1,\n",
    "        cp.sum(x, axis=0, keepdims=True) == 1,\n",
    "        cp.sum(x, axis=1, keepdims=True) <= 1\n",
    "    ]\n",
    "    prob = cp.Problem(obj, con)\n",
    "    prob.solve('GLPK_MI')\n",
    "    index = np.array(np.where(x.value == 1))\n",
    "    # print(real.size,real_full_label,prep_full_label,real_label,prep_label,x.value)\n",
    "    # print(len(real_label),reallogic.shape,preplogic.shape,interset_matrix.shape)\n",
    "    # print(    [np.setdiff1d(real_full_label,\n",
    "    #                     real_label[index[0,:]]),\n",
    "    #        np.setdiff1d(prep_full_label,\n",
    "    #                     prep_label[index[1,:]])])\n",
    "    add_index = np.array([\n",
    "        np.setdiff1d(real_full_label, real_label[index[0, :]]),\n",
    "        np.setdiff1d(prep_full_label, prep_label[index[1, :]])\n",
    "    ],\n",
    "                         dtype=int)\n",
    "    # print(index,add_index)\n",
    "    prep0 = np.setdiff1d(prep_full_label, prep_label[index[1, :]])\n",
    "    # print(prep0, n, n_1)\n",
    "    index = np.concatenate((index, add_index), axis=1)\n",
    "\n",
    "    related_index = []\n",
    "    for i in range(n):\n",
    "        if i < n_1:\n",
    "            real_iter_index = np.where(real == np.unique(real)[index[0, i]])[0]\n",
    "            prep_iter_index = np.where(prep == np.unique(prep)[index[1, i]])[0]\n",
    "            pp_index = np.where(prep_0 == np.unique(prep)[index[1, i]])[0]\n",
    "            related_index.append(pp_index)\n",
    "        else:\n",
    "            real_iter_index = np.where(real == np.unique(real)[index[0, i]])[0]\n",
    "            prep_iter_index = np.where(prep == np.unique(prep0)[i - n_1])[0]\n",
    "            pp_index = np.where(prep_0 == np.unique(prep0)[i - n_1])[0]\n",
    "            related_index.append(pp_index)\n",
    "        error_point_i = np.setdiff1d(real_iter_index, prep_iter_index)\n",
    "        error_point = np.union1d(error_point, error_point_i)\n",
    "    for i in range(n):\n",
    "        prep_0[related_index[i]] = np.ones(len(\n",
    "            related_index[i])) * np.unique(real)[index[0, i]]\n",
    "    error_point = nonnoise_index[error_point]\n",
    "    # error_point = np.union1d(error_point,np.where(prep0==-1)[0])\n",
    "    # error_point = np.union1d(error_point,np.where(real0==-1)[0])\n",
    "    return error_point, prep_0\n",
    "\n",
    "\n",
    "class load_data(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        data = loadmat(dataset)\n",
    "        X = data['data'][0]\n",
    "        self.y = data['truelabel'][0][0].reshape(-1)\n",
    "        self.x = []\n",
    "        if isinstance(X[0], csr_matrix) or isinstance(\n",
    "                X[0], csc_matrix) or isinstance(X[0], coo_matrix):\n",
    "            for i in range(len(X)):\n",
    "                self.x.append(X[i].toarray())\n",
    "        else:\n",
    "            for i in range(len(X)):\n",
    "                self.x.append(X[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(np.array(self.x[idx])), torch.from_numpy(\n",
    "            np.array(self.y[idx])), torch.from_numpy(np.array(idx))\n",
    "\n",
    "\n",
    "def torch_intersect1d(t1: torch.Tensor, t2: torch.Tensor):\n",
    "    # NOTE: requires t1, t2 to be unique 1D Tensor in advance.\n",
    "    # Method: based on unique's count\n",
    "    num_t1, num_t2 = t1.numel(), t2.numel()\n",
    "    u, inv, cnt = torch.unique(torch.cat([t1, t2]),\n",
    "                               return_counts=True,\n",
    "                               return_inverse=True)\n",
    "\n",
    "    cnt_12 = cnt[inv]\n",
    "    cnt_t1, cnt_t2 = cnt_12[:num_t1], cnt_12[num_t1:]\n",
    "    m_t1 = (cnt_t1 == 2)\n",
    "    inds_t1 = m_t1.nonzero()[..., 0]\n",
    "    inds_t1_exclusive = (~m_t1).nonzero()[..., 0]\n",
    "    inds_t2_exclusive = (cnt_t2 == 1).nonzero()[..., 0]\n",
    "\n",
    "    intersection = t1[inds_t1]\n",
    "    t1_exclusive = t1[inds_t1_exclusive]\n",
    "    t2_exclusive = t2[inds_t2_exclusive]\n",
    "    return intersection, t1_exclusive, t2_exclusive\n",
    "\n",
    "\n",
    "def NormalizeData(data):\n",
    "    data = data.T\n",
    "    if np.sum(np.sum(data**2, axis=0) < 0) > 0:\n",
    "        print((data**2)[np.where(np.sum(data**2, axis=0) < 0)[0], :])\n",
    "        print(np.where(np.sum(np.abs(data), axis=0) < 0)[0])\n",
    "        mm = np.maximum(np.sum(np.abs(data), axis=0), 10**-14)\n",
    "\n",
    "        data = data * diags(mm**-1, 0)\n",
    "    else:\n",
    "        mm = np.maximum(np.sum(data**2, axis=0), 10**-14)\n",
    "        data = data * diags(mm**-0.5, 0)\n",
    "    return data.T\n",
    "\n",
    "\n",
    "def comnFun(K, sigma):\n",
    "    nSmp = K[0].shape[0]\n",
    "    view_num = len(K)\n",
    "    KC = np.zeros([nSmp, nSmp])\n",
    "    for i in range(view_num):\n",
    "        KC = KC + sigma[i] * K[i]\n",
    "    return KC\n",
    "\n",
    "\n",
    "def kcenter(K):\n",
    "    n = K.shape[0]\n",
    "    D = np.sum(K, axis=0) / n\n",
    "    E = np.sum(D) / n\n",
    "    J = D.reshape([n, 1])\n",
    "    K = K - J - J.T + E * np.ones([n, n])\n",
    "    K = 0.5 * (K + K.T)\n",
    "    return K\n",
    "\n",
    "\n",
    "def kernel_regularization(K):\n",
    "    G = K + K.T / 2\n",
    "    G = G.detach().cpu().numpy()\n",
    "    D, V = np.linalg.eig(G)\n",
    "    D = D * (D > 10**-14)\n",
    "    G = (V * diags(D, 0)) @ V.T\n",
    "    G = (G + G.T) / 2\n",
    "    return torch.tensor(G).to(device)\n",
    "\n",
    "\n",
    "def generateNeighborhood(X, k):\n",
    "    X = np.copy(X)\n",
    "    knn = []\n",
    "    min_X = np.min(X)\n",
    "    for i in range(k + 1):\n",
    "        index = np.argmax(X, axis=1)\n",
    "        knn.append(index)\n",
    "        # 将近邻元素设置为比对角元素大的值，以便寻找反向k近邻\n",
    "        X[np.arange(X.shape[0]), index] = np.array([min_X - 1] * X.shape[0])\n",
    "    return X < min_X, np.array(knn, dtype=np.int32).T\n",
    "\n",
    "\n",
    "def adapted_neighbor(X, min_k):\n",
    "    X = np.copy(X)\n",
    "    min_X = np.min(X)\n",
    "    for knn_k in range(int(0.5 * X.shape[0])):\n",
    "        index = np.argmax(X, axis=1)\n",
    "        X[np.arange(X.shape[0]), index] = np.array([min_X - 1] * X.shape[0])\n",
    "        logic_nn = X < min_X\n",
    "        logic_nn = logic_nn * logic_nn.T\n",
    "        if np.mean(np.sum(logic_nn, axis=1)) > min_k:\n",
    "            print((np.sum(np.sum(logic_nn, axis=1)) / X.shape[0]), knn_k)\n",
    "            return X < min_X\n",
    "        if knn_k >= max(int(0.02 * X.shape[0]), 5):\n",
    "            print((np.sum(np.sum(logic_nn, axis=1)) / X.shape[0]), knn_k,\n",
    "                  max(int(0.01 * X.shape[0]), 5))\n",
    "            return X < min_X\n",
    "    return X < min_X\n",
    "\n",
    "\n",
    "def knn_mean_X(X, knn):\n",
    "    knn_X = X[knn]\n",
    "    return np.sum(knn_X, axis=1) / np.maximum(np.sum(knn_X > 0, axis=1),\n",
    "                                              np.ones(X[0].shape))\n",
    "\n",
    "\n",
    "def compute_init_loss(K, view_K, X_bar, model):\n",
    "    loss0 = 0\n",
    "    loss1 = 0\n",
    "    loss_param = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    for i in range(len(view_K)):\n",
    "        for param in model[i].parameters():\n",
    "            if param is model[i].kernel_weights:\n",
    "                continue\n",
    "            loss_param += torch.norm(param, p=1) / max(param.size())\n",
    "        loss0 = loss0 + criterion(view_K[i].view(-1), K[i].view(-1))\n",
    "        loss1 = loss1 + torch.sum(\n",
    "            torch.log1p(X_bar[i]) + torch.log1p(1 - X_bar[i]))\n",
    "    return [loss0, loss1, loss_param]\n",
    "\n",
    "\n",
    "def kernelkmeans(K, n_clusters):\n",
    "    _, H = eigsh(K, k=n_clusters, which='LA')\n",
    "    # U, _, _ = np.linalg.svd(K)\n",
    "    # H = U[:, 0:n_clusters]\n",
    "    return H\n",
    "\n",
    "\n",
    "def data_sample(X, K, Smp_num):\n",
    "    n_total = X[0].shape[0]\n",
    "    Smp_index = torch.randint(0, n_total, [\n",
    "        Smp_num,\n",
    "    ])\n",
    "    Smp_X = []\n",
    "    Smp_K = []\n",
    "    grid = torch.meshgrid(Smp_index, Smp_index)\n",
    "    for i in range(len(X)):\n",
    "        n_feature = X[i].shape[1]\n",
    "        Smp_X.append(K[i][Smp_index, :])\n",
    "        Smp_K.append(K[i][grid[0], grid[1]])\n",
    "    return Smp_X, Smp_K, Smp_index\n",
    "\n",
    "\n",
    "def selfNN(X, options):\n",
    "    nozero_number = torch.minimum(\n",
    "        torch.ones(X.shape[1], dtype=torch.int32, device=device) *\n",
    "        min([4, int(X.shape[1] / options.n_clusters)]), torch.sum(X > 0,\n",
    "                                                                  dim=1))\n",
    "    delete_index = torch.sort(X, dim=1).values[torch.arange(X.shape[1]),\n",
    "                                               -nozero_number]\n",
    "    return (1. * (X >= delete_index) +\n",
    "            (torch.eye(X.shape[1], dtype=torch.int32, device=device))) > 0\n",
    "\n",
    "\n",
    "class connectivity(nn.Module):\n",
    "\n",
    "    def __init__(self, options):\n",
    "        super(connectivity, self).__init__()\n",
    "        self.options = options\n",
    "        self.activ = torch.nn.Sigmoid()\n",
    "        self.activ1 = torch.nn.ReLU()\n",
    "        self.activ2 = torch.nn.Tanh()\n",
    "        self.matrix = torch.nn.Parameter(\n",
    "            1 / (options.nSmp) *\n",
    "            torch.zeros([options.nSmp, options.nSmp], dtype=torch.float32))\n",
    "        self.layerencode = nn.ModuleList([\n",
    "            nn.Linear(options.layer_width_c[i],\n",
    "                      options.layer_width_c[i + 1],\n",
    "                      bias=False,\n",
    "                      dtype=torch.float32)\n",
    "            for i in range(len(options.layer_width_c) - 1)\n",
    "        ])\n",
    "\n",
    "        for i in range(len(options.layer_width_c) - 1):\n",
    "            nn.init.kaiming_normal_(self.layerencode[i].weight,\n",
    "                                    nonlinearity='relu')\n",
    "            # nn.init.orthogonal_(self.layerencode[i].weight.T)\n",
    "\n",
    "    def forward(self, knn_list):\n",
    "        X = []\n",
    "        for i in range(self.options.view_num):\n",
    "            X.append(knn_list[i])\n",
    "            for j in range(len(self.layerencode)):\n",
    "                if j < len(self.layerencode) - 1:\n",
    "                    X[i] = self.activ1(X[i] @ self.layerencode[j].weight.T)\n",
    "                else:\n",
    "                    X[i] = X[i] @ self.layerencode[j].weight.T\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "def init_Kernel_train(y, view_K, options):\n",
    "    knn_list = []\n",
    "    optimizers = {}\n",
    "    init_maxiter = options.maxIter\n",
    "    # print('0', torch.cuda.memory_allocated() / 2**20)\n",
    "    average_K = comnFun(view_K, np.ones(options.view_num) / options.view_num)\n",
    "    np.savetxt('KNN_K.npy', average_K )\n",
    "    for i in range(options.view_num):\n",
    "        knn_list.append(\n",
    "            torch.tensor(view_K[i], dtype=torch.float32, device=device))\n",
    "    connect_kernel = connectivity(options).to(device)\n",
    "\n",
    "    optimizers[\"optimizer_connectivity\"] = optim.Adam(\n",
    "        [i.weight for i in connect_kernel.layerencode], lr=1e-3)\n",
    "    singular_value = [\n",
    "        torch.ones(options.n_clusters,\n",
    "                   dtype=torch.float32,\n",
    "                   device=device,\n",
    "                   requires_grad=True) for i in range(options.view_num)\n",
    "    ]\n",
    "    optimizers[\"optimizer_singular\"] = optim.Adam(singular_value, lr=1e-3)\n",
    "\n",
    "    best_label = np.zeros([3, options.nSmp], dtype=np.int32)\n",
    "    best_accurancy = [0] * 3\n",
    "    knn_temp_list = []\n",
    "    knn_temp_max_list = []\n",
    "    knn_temp_min_list = []\n",
    "    # print('K', torch.cuda.memory_allocated() / 2**20)\n",
    "    for i in range(options.view_num):\n",
    "        logic_knn = torch.tensor(1. * options.knn[i],\n",
    "                                 dtype=torch.float32,\n",
    "                                 device=device)\n",
    "        knn_temp_list.append(knn_list[i] * (1. *\n",
    "                                            (logic_knn + logic_knn.T) > 0))\n",
    "        # print('K' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        knn_temp_min_list.append(knn_list[i] * (1. *\n",
    "                                                (logic_knn * logic_knn.T) > 0))\n",
    "        # print('K_min' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        del logic_knn\n",
    "        torch.cuda.empty_cache()\n",
    "        D = ((torch.maximum(\n",
    "            torch.abs(torch.sum(knn_temp_min_list[i], dim=1)),\n",
    "            torch.ones(knn_temp_min_list[i].shape[0],\n",
    "                       dtype=torch.float32).to(device) *\n",
    "            1e-15)**-1)**.5).unsqueeze(1)\n",
    "        knn_temp_min_list[i] = knn_temp_min_list[i] * D * D.T\n",
    "        # print('K_min' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        D = ((torch.maximum(\n",
    "            torch.abs(torch.sum(knn_temp_list[i], dim=1)),\n",
    "            torch.ones(knn_temp_list[i].shape[0],\n",
    "                       dtype=torch.float32).to(device) *\n",
    "            1e-15)**-1)**.5).unsqueeze(1)\n",
    "        knn_temp_list[i] = knn_temp_list[i] * D * D.T\n",
    "        # print('K' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        knn_temp_max_list.append(knn_temp_list[i] @ knn_temp_list[i])\n",
    "        # print('K_max' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        knn_temp_max_list[i] = torch.maximum(\n",
    "            torch.minimum(knn_temp_max_list[i] * 1. * (knn_temp_list[i] > 0),\n",
    "                          knn_temp_list[i]), knn_temp_max_list[i])\n",
    "        # print('K_max' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "        print(\n",
    "            torch.sum(torch.sum(knn_temp_list[i] > 0, dim=1)) / options.nSmp,\n",
    "            torch.sum(torch.sum(knn_temp_min_list[i] > 0, dim=1)) /\n",
    "            options.nSmp,\n",
    "            torch.sum(torch.sum(knn_temp_max_list[i] > 0, dim=1)) /\n",
    "            options.nSmp)\n",
    "        # print('3K' + str(i), torch.cuda.memory_allocated() / 2**20)\n",
    "    del D, knn_list\n",
    "    torch.cuda.empty_cache()\n",
    "    if options.alpha == 0:\n",
    "        del knn_temp_min_list\n",
    "        torch.cuda.empty_cache()\n",
    "    if options.beta == 0:\n",
    "        del knn_temp_max_list\n",
    "        torch.cuda.empty_cache()\n",
    "    # print('init', torch.cuda.memory_allocated() / 2**20)\n",
    "    old_loss = 1e15\n",
    "    old_rank = 0\n",
    "    old_inter_loss = -1e15\n",
    "    object_function = np.zeros(options.maxIter - options.init_time)\n",
    "    for epoch in range(init_maxiter):\n",
    "        loss_init = 0\n",
    "        loss_list = []\n",
    "        reg_loss = 0\n",
    "        start = time.time()\n",
    "        X_view = connect_kernel(knn_temp_list)\n",
    "        for j in range(len(options.layer_width_c) - 1):\n",
    "            reg_loss += 0.5 * torch.norm(\n",
    "                connect_kernel.layerencode[j].weight)**2\n",
    "        print(reg_loss.item())\n",
    "        # print('H', torch.cuda.memory_allocated() / 2**20)\n",
    "        orx_loss = []\n",
    "        connected_matrix = (connect_kernel.matrix +\n",
    "                            connect_kernel.matrix.T) / 2\n",
    "        # print('K_star', torch.cuda.memory_allocated() / 2**20)\n",
    "        view_loss = []\n",
    "        connect_loss = []\n",
    "        view_loss_temp = []\n",
    "        view_loss_temp_1 = []\n",
    "        eta = []\n",
    "        X_view_orth_grad = torch.zeros_like(X_view[0],\n",
    "                                            device=device,\n",
    "                                            dtype=torch.float32)\n",
    "        X_view_grad = torch.zeros_like(X_view[0],\n",
    "                                       device=device,\n",
    "                                       dtype=torch.float32)\n",
    "        print('forward_time', time.time() - start)\n",
    "        for i in range(options.view_num):\n",
    "            # X_view[i].retain_grad()\n",
    "            start = time.time()\n",
    "            connect_view_k = ((X_view[i] * singular_value[i]) @ X_view[i].T)\n",
    "            # print('K_c', torch.cuda.memory_allocated() / 2**20)\n",
    "            # orx_loss.append(\n",
    "            #     torch.norm((or_matrix) - torch.eye(\n",
    "            #         options.n_clusters, dtype=torch.float16).to(device)) +\n",
    "            #     torch.norm(connect_view_k_power - connect_view_k) +\n",
    "            #     torch.norm(connect_view_k - connect_view_k_pos))\n",
    "            orx_loss.append(\n",
    "                torch.norm(X_view[i].T @ X_view[i] - torch.eye(\n",
    "                    options.n_clusters, dtype=torch.float32, device=device))**\n",
    "                2)\n",
    "            # print('orx_loss', torch.cuda.memory_allocated() / 2**20)\n",
    "            X_view_orth_grad = 4 * (X_view[i] @ X_view[i].T @ X_view[i] -\n",
    "                                    X_view[i])\n",
    "            # print('X_view_orth_grad', torch.cuda.memory_allocated() / 2**20)\n",
    "            # view_loss_temp_i = 1 / (1 + options.alpha +\n",
    "            #                         options.beta) * torch.norm(\n",
    "            #                             (knn_temp_list[i] - connect_view_k))**2\n",
    "\n",
    "            # view_loss_temp.append(1 / (1 + options.alpha + options.beta) *\n",
    "            #                       torch.norm(\n",
    "            #                           (knn_temp_list[i] - connect_view_k))**2)\n",
    "            view_loss_temp.append(\n",
    "                1 / (1 + options.alpha + options.beta) *\n",
    "                torch.trace(X_view[i].T @ (X_view[i] * singular_value[i])\n",
    "                            @ X_view[i].T @ (X_view[i] * singular_value[i]) -\n",
    "                            2 * X_view[i].T @ knn_temp_list[i] @ (\n",
    "                                X_view[i] * singular_value[i])))\n",
    "            # print('view_loss_temp', torch.cuda.memory_allocated() / 2**20)\n",
    "            X_view_grad = 2 / (1 + options.alpha + options.beta) * (\n",
    "                2 * connect_view_k - knn_temp_list[i] -\n",
    "                knn_temp_list[i].T) @ (X_view[i] * singular_value[i])\n",
    "            # print('X_view_grad', torch.cuda.memory_allocated() / 2**20)\n",
    "            if options.alpha > 0:\n",
    "                # view_loss_temp_1.append(\n",
    "                #     options.alpha / (1 + options.alpha + options.beta) *\n",
    "                #     torch.norm(knn_temp_min_list[i] - (\n",
    "                #         (X_view[i] * singular_value[i]**0.5) @ X_view[i].T))**\n",
    "                #     2)\n",
    "                view_loss_temp_1.append(\n",
    "                    options.alpha / (1 + options.alpha + options.beta) *\n",
    "                    torch.trace(\n",
    "                        X_view[i].T @ (X_view[i] * singular_value[i]**.5)\n",
    "                        @ X_view[i].T @ (X_view[i] * singular_value[i]**.5) -\n",
    "                        2 * X_view[i].T @ knn_temp_min_list[i] @ (\n",
    "                            X_view[i] * singular_value[i]**.5)))\n",
    "                # print('view_loss_temp_1',\n",
    "                #       torch.cuda.memory_allocated() / 2**20)\n",
    "                X_view_grad += 2 * options.alpha / (\n",
    "                    1 + options.alpha + options.beta) * (\n",
    "                        2 *\n",
    "                        (X_view[i] * singular_value[i]**0.5) @ X_view[i].T -\n",
    "                        knn_temp_min_list[i] - knn_temp_min_list[i].T) @ (\n",
    "                            X_view[i] * singular_value[i]**.5)\n",
    "            if options.beta > 0:\n",
    "                view_loss_temp_1.append(\n",
    "                    options.alpha / (1 + options.alpha + options.beta) *\n",
    "                    torch.trace(\n",
    "                        X_view[i].T @ (X_view[i] * singular_value[i]**2)\n",
    "                        @ X_view[i].T @ (X_view[i] * singular_value[i]**2) -\n",
    "                        2 * X_view[i].T @ knn_temp_max_list[i] @ (\n",
    "                            X_view[i] * singular_value[i]**2)))\n",
    "\n",
    "                X_view_grad += 2 * options.beta / (\n",
    "                    1 + options.alpha + options.beta) * (\n",
    "                        2 * (X_view[i] * singular_value[i]**2) @ X_view[i].T -\n",
    "                        knn_temp_max_list[i] -\n",
    "                        knn_temp_max_list[i].T) @ (X_view[i] *\n",
    "                                                   (singular_value[i]**2))\n",
    "            # print('before_connect' + str(i),\n",
    "            #       torch.cuda.memory_allocated() / 2**20)\n",
    "            print('view_time:', time.time() - start)\n",
    "            if (epoch >= options.init_time):\n",
    "                start = time.time()\n",
    "                diag = (torch.diag(\n",
    "                    connect_view_k +\n",
    "                    torch.relu(connected_matrix)).detach().unsqueeze(1))**.5\n",
    "                # print(diag.dtype)\n",
    "                # print('diag', torch.cuda.memory_allocated() / 2**20)\n",
    "                connectedness_matrix = (\n",
    "                    (connect_view_k + torch.relu(connected_matrix)) / diag /\n",
    "                    diag.T).detach().clone()\n",
    "                del diag\n",
    "                torch.cuda.empty_cache()\n",
    "                connectedness = torch.sum(connectedness_matrix,\n",
    "                                          dim=1).unsqueeze(1)\n",
    "\n",
    "                neighbor_peak = torch.tensor(\n",
    "                    1, device=device,\n",
    "                    dtype=torch.float32) * (connectedness.repeat(\n",
    "                        1, options.nSmp) < connectedness.T)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                # print('neighbor_peak', torch.cuda.memory_allocated() / 2**20)\n",
    "                neighbor_peak = neighbor_peak * (knn_temp_list[i] > 0)\n",
    "                neighbor_peak_index = torch.where(\n",
    "                    torch.sum(neighbor_peak, dim=1) == 0)[0]\n",
    "                # print('neighbor_peak_index',\n",
    "                #       torch.cuda.memory_allocated() / 2**20)\n",
    "                parents_index = neighbor_peak * (connectedness_matrix)\n",
    "                if len(neighbor_peak_index) < options.n_clusters:\n",
    "                    all_index = torch.arange(options.nSmp,\n",
    "                                             dtype=torch.int32,\n",
    "                                             device=device)\n",
    "                    rest_index = torch_intersect1d(all_index,\n",
    "                                                   neighbor_peak_index)[1]\n",
    "                    rest_index = (torch.sort(connectedness[rest_index])[1]\n",
    "                                  [-options.n_clusters +\n",
    "                                   len(neighbor_peak_index):]).squeeze(1)\n",
    "                    neighbor_peak_index = torch.concatenate(\n",
    "                        [neighbor_peak_index, rest_index], dim=0)\n",
    "                    print(\"_______________\", len(neighbor_peak_index),\n",
    "                          len(rest_index))\n",
    "                    del rest_index, all_index\n",
    "                del neighbor_peak, connectedness_matrix, connectedness\n",
    "                torch.cuda.empty_cache()\n",
    "                parents_index[neighbor_peak_index,\n",
    "                              neighbor_peak_index] = torch.ones(\n",
    "                                  len(neighbor_peak_index),\n",
    "                                  device=device,\n",
    "                                  dtype=torch.float32)\n",
    "                del neighbor_peak_index\n",
    "                torch.cuda.empty_cache()\n",
    "                # print('parents_index', torch.cuda.memory_allocated() / 2**20)\n",
    "                D = ((torch.maximum(\n",
    "                    torch.abs(torch.sum(parents_index, dim=1)),\n",
    "                    torch.ones(knn_temp_list[i].shape[0],\n",
    "                               device=device,\n",
    "                               dtype=torch.float32) *\n",
    "                    1e-15))).detach().unsqueeze(1)\n",
    "                # print('D', torch.cuda.memory_allocated() / 2**20)\n",
    "                parents_index = (parents_index / D)\n",
    "                print('peak_time:', time.time() - start)\n",
    "                # connect_loss.append(\n",
    "                #     torch.norm(parents_index - (X_view[i] @ X_view[i].T))**2)\n",
    "                connect_loss.append(1 * torch.trace(\n",
    "                    X_view[i].T @ X_view[i] @ X_view[i].T @ X_view[i] -\n",
    "                    X_view[i].T @ (parents_index + parents_index.T) @ X_view[i]\n",
    "                ))\n",
    "                X_view_grad += 1 * 2 * options.gamma * (\n",
    "                    2 * (X_view[i] @ X_view[i].T) - parents_index -\n",
    "                    parents_index.T) @ X_view[i]\n",
    "                del parents_index, D\n",
    "                torch.cuda.empty_cache()\n",
    "                connect_loss.append(\n",
    "                    options.con * (1 + options.gamma) / options.gamma *\n",
    "                    torch.trace(\n",
    "                        X_view[i].T @ (X_view[i] * singular_value[i])\n",
    "                        @ X_view[i].T @ (X_view[i] * singular_value[i]) -\n",
    "                        2 * X_view[i].T @ connected_matrix.detach().clone() @ (\n",
    "                            X_view[i] * singular_value[i])))\n",
    "                X_view_grad += 4 * options.con * (1 + options.gamma) * (\n",
    "                    connect_view_k - connected_matrix) @ (X_view[i] *\n",
    "                                                          singular_value[i])\n",
    "\n",
    "                if torch.trace(connect_view_k) > options.n_clusters:\n",
    "                    connect_loss.append(\n",
    "                        (1 + options.gamma) / options.gamma *\n",
    "                        (torch.trace(\n",
    "                            X_view[i].T @ (X_view[i] * singular_value[i])) -\n",
    "                         options.n_clusters))\n",
    "                    X_view_grad += 2 * (1 + options.gamma) * (\n",
    "                        X_view[i] * singular_value[i])\n",
    "                elif torch.trace(connect_view_k) < options.n_clusters:\n",
    "                    connect_loss.append(\n",
    "                        (1 + options.gamma) / options.gamma *\n",
    "                        (options.n_clusters - torch.trace(\n",
    "                            X_view[i].T @ (X_view[i] * singular_value[i]))))\n",
    "                    X_view_grad -= 2 * (1 + options.gamma) * (\n",
    "                        X_view[i] * singular_value[i])\n",
    "                print(\"peak\", torch.cuda.memory_allocated() / 2**20)\n",
    "                start = time.time()\n",
    "                norm_1 = torch.norm(X_view_grad, dim=0).detach().clone()\n",
    "                norm_2 = torch.norm(X_view_orth_grad, dim=0).detach().clone()\n",
    "                cos_value = torch.mean(\n",
    "                    torch.abs(torch.diag(X_view_orth_grad.T @ X_view_grad)) /\n",
    "                    norm_1 / norm_2).item()\n",
    "                if cos_value > options.eta:\n",
    "                    eta.append(\n",
    "                        min(options.eta / cos_value * (1 + options.gamma),\n",
    "                            min(norm_1 / norm_2)))\n",
    "                else:\n",
    "                    eta.append(min(1 + options.gamma, min(norm_1 / norm_2)))\n",
    "                print('eta_time:', time.time() - start)\n",
    "        del X_view_grad, X_view_orth_grad\n",
    "        torch.cuda.empty_cache()\n",
    "        view_loss.append(torch.stack(view_loss_temp))\n",
    "        if len(view_loss_temp_1) > 0:\n",
    "            view_loss.append(torch.stack(view_loss_temp_1))\n",
    "        del view_loss_temp, view_loss_temp_1\n",
    "        torch.cuda.empty_cache()\n",
    "        start = time.time()\n",
    "        orx_loss = torch.stack(orx_loss)\n",
    "        if len(connect_loss) > 0:\n",
    "            eta = torch.tensor(eta, device=device, dtype=torch.float32)\n",
    "            view_loss = torch.stack(view_loss)\n",
    "            connect_loss = torch.stack(connect_loss)\n",
    "            if torch.isnan(torch.sum(view_loss) + torch.sum(connect_loss)):\n",
    "                print(view_loss, connect_loss)\n",
    "                break\n",
    "            loss_init = torch.sum(eta * orx_loss) + torch.sum(\n",
    "                view_loss) + options.gamma * torch.sum(connect_loss)\n",
    "            loss_list.append([\n",
    "                orx_loss,\n",
    "                view_loss,\n",
    "                connect_loss,\n",
    "            ])\n",
    "\n",
    "            # for optimizer in optimizers.values():\n",
    "            #     optimizer.zero_grad()\n",
    "            if ((torch.sum(orx_loss)\n",
    "                 < 1e-3 * options.view_num * options.n_clusters) and\n",
    "                (abs(old_loss.item() - loss_init.item()) < abs(\n",
    "                    loss_init.item()) * 1e-5)) or epoch == options.maxIter - 1:\n",
    "                print(torch.sum(orx_loss),\n",
    "                      1e-3 * options.view_num * options.n_clusters**2)\n",
    "                H = kernelkmeans((connected_matrix).detach().to(\n",
    "                    torch.float32).cpu().numpy(), options.n_clusters)\n",
    "                # H_normalized = H\n",
    "                H_normalized = H / (np.sum(H**2, axis=1)**0.5).reshape(\n",
    "                    [H.shape[0], 1])\n",
    "                kmeans_model = KMeans(n_clusters=options.n_clusters,\n",
    "                                      n_init='auto')\n",
    "\n",
    "                repeat = 50\n",
    "                best_inertia = np.zeros([3, repeat])\n",
    "                for rep in range(repeat):\n",
    "                    kmeans = kmeans_model.fit(H_normalized)\n",
    "                    try:\n",
    "                        y_prep = error_point(kmeans.labels_, y)[1]\n",
    "                    except Exception as e:\n",
    "                        y_prep = kmeans.labels_\n",
    "                    y_prep = y_prep.astype('int')\n",
    "                    ari = adjusted_rand_score(y, y_prep)\n",
    "                    nmi = normalized_mutual_info_score(y, y_prep)\n",
    "                    acc = accuracy_score(y, y_prep)\n",
    "                    if ari > best_accurancy[0]:\n",
    "                        best_label[0, :] = y_prep\n",
    "                    if nmi > best_accurancy[1]:\n",
    "                        best_label[1, :] = y_prep\n",
    "                    if acc > best_accurancy[2]:\n",
    "                        best_label[2, :] = y_prep\n",
    "                    best_inertia[:, rep] = np.array([ari, nmi, acc])\n",
    "                print(np.std(best_inertia, axis=1), np.max(best_inertia,\n",
    "                                                           axis=1),\n",
    "                      np.min(best_inertia, axis=1))\n",
    "                best_inertia = (np.max(best_inertia, axis=1)).tolist()\n",
    "                best_accurancy = np.maximum(best_inertia,\n",
    "                                            best_accurancy).tolist()\n",
    "\n",
    "                return best_accurancy, best_label, best_inertia, np.array(\n",
    "                    object_function)\n",
    "        else:\n",
    "            view_loss = torch.stack(view_loss)\n",
    "            loss_init = torch.sum(view_loss)\n",
    "            loss_list.append([orx_loss, view_loss])\n",
    "            connect_kernel.matrix = torch.nn.Parameter(\n",
    "                sum([((X_view[i] * singular_value[i])\n",
    "                      @ X_view[i].T).detach().clone()\n",
    "                     for i in range(options.view_num)]) / options.view_num)\n",
    "            # print('K_star', torch.cuda.memory_allocated() / 2**20)\n",
    "            optimizers[\"optimizer_matrix\"] = optim.Adam(\n",
    "                [connect_kernel.matrix], lr=1e-3)\n",
    "        if torch.isnan(loss_init):\n",
    "            print(loss_list)\n",
    "            print((torch.diag(connect_view_k + connected_matrix)**0.5\n",
    "                   ).unsqueeze(1).detach().clone(), )\n",
    "            break\n",
    "\n",
    "        # back_time = time.time()\n",
    "        # print(\"back_time---------------------\", time.time() - back_time)\n",
    "        (loss_init + options.regular * reg_loss).backward()\n",
    "        if len(connect_loss) > 0:\n",
    "            object_function[epoch - options.init_time] = loss_init.item(\n",
    "            ) + options.view_num * options.gamma * torch.trace(\n",
    "                connected_matrix @ connected_matrix).item()\n",
    "            # back_time1 = time.time()\n",
    "            # torch.sum(view_loss).backward(retain_graph=True)\n",
    "            # print(time.time() - back_time1)\n",
    "            # back_time1 = time.time()\n",
    "            # torch.sum(eta * orx_loss).backward(retain_graph=True)\n",
    "            # print(time.time() - back_time1)\n",
    "            # back_time1 = time.time()\n",
    "            # (options.gamma *\n",
    "            #  torch.sum(connect_loss)).backward(retain_graph=False)\n",
    "            # print(time.time() - back_time1)\n",
    "            connect_kernel.matrix.grad = options.con * (\n",
    "                1 + options.gamma) * options.view_num * (\n",
    "                    4 *\n",
    "                    (connected_matrix @ connected_matrix @ connected_matrix) -\n",
    "                    6 * (connected_matrix @ connected_matrix) +\n",
    "                    2 * connected_matrix).detach().clone()\n",
    "            for i in range(options.view_num):\n",
    "                connect_kernel.matrix.grad += options.con * (\n",
    "                    1 + options.gamma) * (2 * connected_matrix - 2 *\n",
    "                                          (X_view[i] * singular_value[i])\n",
    "                                          @ X_view[i].T).detach().clone()\n",
    "            # loss_init.backward(retain_graph=False)\n",
    "        # else:\n",
    "        #     torch.sum(view_loss).backward(retain_graph=False)\n",
    "\n",
    "        # back_time = time.time()\n",
    "        # for i in range(options.view_num):\n",
    "        #     for j in range(len(options.layer_width_c) - 1):\n",
    "        #         connect_kernel.layerencode[j].weight.grad = torch.autograd.grad(loss_init, connect_kernel.layerencode[j].weight, retain_graph=True)[0]\n",
    "        #     singular_value[i].grad = torch.autograd.grad(loss_init, singular_value[i], retain_graph=True)[0]\n",
    "        # if len(connect_loss)>0:\n",
    "        #     connect_kernel.matrix.grad = torch.autograd.grad(loss_init, connect_kernel.matrix, retain_graph=False)[0]\n",
    "        print(\"back_time\", time.time() - start)\n",
    "        old_loss = loss_init\n",
    "        if (epoch) % 25 == 0:\n",
    "            print(epoch, \":\")\n",
    "            print('total_loss:\\n', loss_init.item())\n",
    "            print('one_loss:\\n', loss_list)\n",
    "            print(\"connectivity_matrix:\")\n",
    "            new_rank = torch.trace(connected_matrix).item() / torch.mean(\n",
    "                torch.stack(singular_value)).item()\n",
    "            print(new_rank)\n",
    "            print(singular_value)\n",
    "            print('---------------------')\n",
    "        if ((epoch) % 25 == 0) and epoch != 0:\n",
    "            if epoch == options.init_time:\n",
    "                np.savetxt('consensus_kernel_before.npy',\n",
    "                           (connected_matrix).detach().to(\n",
    "                               torch.float32).cpu().numpy())\n",
    "            if new_rank > 0.9 * old_rank and epoch >= options.init_time:\n",
    "                start = time.time()\n",
    "                np.savetxt('consensus_kernel_after.npy',\n",
    "                           (connected_matrix).detach().to(\n",
    "                               torch.float32).cpu().numpy())\n",
    "                old_rank = min(new_rank, options.n_clusters)\n",
    "                H = kernelkmeans((connected_matrix).detach().to(\n",
    "                    torch.float32).cpu().numpy(), options.n_clusters)\n",
    "                H_normalized = H / (np.sum(H**2, axis=1)**0.5).reshape(\n",
    "                    [H.shape[0], 1])\n",
    "                kmeans_model = KMeans(n_clusters=options.n_clusters,\n",
    "                                      n_init='auto')\n",
    "\n",
    "                repeat = 50\n",
    "                best_inertia = np.zeros([3, repeat])\n",
    "                for rep in range(repeat):\n",
    "                    kmeans = kmeans_model.fit(H_normalized)\n",
    "                    try:\n",
    "                        y_prep = error_point(kmeans.labels_, y)[1]\n",
    "                    except Exception as e:\n",
    "                        y_prep = kmeans.labels_\n",
    "                    y_prep = y_prep.astype('int')\n",
    "                    ari = adjusted_rand_score(y, y_prep)\n",
    "                    nmi = normalized_mutual_info_score(y, y_prep)\n",
    "                    acc = accuracy_score(y, y_prep)\n",
    "                    if ari > best_accurancy[0]:\n",
    "                        best_label[0, :] = y_prep\n",
    "                    if nmi > best_accurancy[1]:\n",
    "                        best_label[1, :] = y_prep\n",
    "                    if acc > best_accurancy[2]:\n",
    "                        best_label[2, :] = y_prep\n",
    "                    best_inertia[:, rep] = np.array([ari, nmi, acc])\n",
    "                print(np.std(best_inertia, axis=1), np.max(best_inertia,\n",
    "                                                           axis=1),\n",
    "                      np.min(best_inertia, axis=1),\n",
    "                      np.mean(best_inertia, axis=1))\n",
    "                best_inertia = (np.max(best_inertia, axis=1)).tolist()\n",
    "                best_accurancy = np.maximum(best_inertia,\n",
    "                                            best_accurancy).tolist()\n",
    "\n",
    "                print(\"total\", best_inertia, best_accurancy)\n",
    "                print('clustering_time', time.time() - start)\n",
    "            elif new_rank < 0.9 * old_rank and old_loss > old_inter_loss:\n",
    "                return best_accurancy, best_label, best_inertia, np.array(\n",
    "                    object_function)\n",
    "\n",
    "            for i in range(options.view_num):\n",
    "                ari = adjusted_rand_score(\n",
    "                    y,\n",
    "                    np.argmax(X_view[i].detach().to(\n",
    "                        torch.float32).cpu().numpy(),\n",
    "                              axis=1))\n",
    "                nmi = normalized_mutual_info_score(\n",
    "                    y,\n",
    "                    np.argmax(X_view[i].detach().to(\n",
    "                        torch.float32).cpu().numpy(),\n",
    "                              axis=1))\n",
    "                print(\"view_\" + str(i), ari, nmi)\n",
    "\n",
    "            print('---------------------')\n",
    "        for optimizer_key, optimizer_value in zip(optimizers.keys(),\n",
    "                                                  optimizers.values()):\n",
    "            optimizer_value.step()\n",
    "            optimizer_value.zero_grad()\n",
    "        connect_kernel.matrix.grad = None\n",
    "        torch.cuda.empty_cache()\n",
    "    return best_accurancy, best_label, best_inertia, np.array(object_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (165, 1024)\n",
      "3.1333333333333333 4\n",
      "3.4 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "\n",
    "datasets_path = \"./datasets\"\n",
    "file_name = \"YALE.mat\"\n",
    "data = load_data(datasets_path + '/' + file_name)\n",
    "torch.cuda.empty_cache()\n",
    "EmptyStruct = type('EmptyStruct', (), {})\n",
    "options = EmptyStruct()\n",
    "n_clusters = len(np.unique(data.y))\n",
    "options.n_clusters = n_clusters\n",
    "view_num = len(data.x)\n",
    "\n",
    "options.view_num = view_num\n",
    "nSmp = data.x[0].shape[0]\n",
    "options.nSmp = nSmp\n",
    "options.layer_width_c = [nSmp, max([min([int(nSmp**0.5),int(nSmp/n_clusters)]), n_clusters]), n_clusters]\n",
    "options.init_time = 300\n",
    "options.maxIter = 600\n",
    "Sigma = np.ones(view_num) / view_num\n",
    "options.threshold = 0.45\n",
    "options.regular = 1e-2\n",
    "options.con = 1\n",
    "print(view_num, data.x[0].shape)\n",
    "\n",
    "options.knn = []\n",
    "options.min_k = 3\n",
    "# init kernel matrix\n",
    "K = []\n",
    "normal_X = []\n",
    "for i in range(view_num):\n",
    "    if file_name[-5] == 'K':\n",
    "        TempK = data.x[i]\n",
    "        TempK = kcenter(TempK)\n",
    "        TempK = TempK / (np.diag(TempK)**0.5).reshape([options.nSmp, 1]) / (\n",
    "            np.diag(TempK)**0.5).reshape([options.nSmp, 1]).T\n",
    "        TempD = np.abs(np.sum(TempK, axis=1, keepdims=True))**-.5\n",
    "        V, D = eigsh(TempK * TempD * TempD.T, k=options.n_clusters, which='LA')\n",
    "        print(i, V)\n",
    "        K.append(TempK / V[-1])\n",
    "        options.knn.append(adapted_neighbor(TempK / V[-1], options.min_k))\n",
    "        del TempK\n",
    "    elif data.x[i].shape[1]==data.x[i].shape[0]:\n",
    "        print('distance_data')\n",
    "        TempK = data.x[i]\n",
    "        t = np.mean(np.mean(TempK))\n",
    "        TempK = np.exp(-TempK**2 / (2 * t**2))\n",
    "        K.append(TempK)\n",
    "        options.knn.append(adapted_neighbor(TempK, options.min_k))\n",
    "        del TempK\n",
    "    else:\n",
    "        temp_x = NormalizeData(data.x[i])\n",
    "        TempK = pairwise_distances(temp_x, metric='euclidean')\n",
    "        # logic_knn, knn = generateNeighborhood(-TempK, knn_k)\n",
    "        t = np.mean(np.mean(TempK))\n",
    "        TempK = np.exp(-TempK**2 / (2 * t**2))\n",
    "        K.append(TempK)\n",
    "        options.knn.append(adapted_neighbor(TempK, options.min_k))\n",
    "        del TempK, temp_x  #mean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.alpha = 1\n",
    "options.beta = 0\n",
    "options.eta = 0.05\n",
    "final_accurancy = np.zeros([71, 6])\n",
    "loss_value = np.zeros([71, options.maxIter - options.init_time])\n",
    "# final_accurancy = np.loadtxt('results/' + file_name[:-4] + \"_\" +\n",
    "#                              str(options.eta) + \".npy\")\n",
    "# loss_value = np.loadtxt('results/' + file_name[:-4] + \"loss_value_\" + \"_\" +\n",
    "#             str(options.eta) + \".npy\")\n",
    "if final_accurancy.shape[0]<71:\n",
    "    final_accurancy = np.concatenate([final_accurancy,np.zeros([71-final_accurancy.shape[0],6])],axis=0)\n",
    "if loss_value.shape[0]<71:\n",
    "    loss_value = np.concatenate([loss_value,np.zeros([71-loss_value.shape[0],options.maxIter - options.init_time])],axis=0)\n",
    "# loss_value = np.loadtxt('results/' + file_name[:-4] + \"loss_value_\" + \"_\" +\n",
    "#             str(options.eta) + \".npy\")\n",
    "# rewrite_index = np.array([25])\n",
    "# final_accurancy[rewrite_index, :] = np.zeros([len(rewrite_index), 6])\n",
    "t = 0\n",
    "for num in range(-10, 11, 1):\n",
    "    options.gamma = 2**(num)\n",
    "    torch.cuda.empty_cache()\n",
    "    if np.sum(final_accurancy[t, :]) == 0:\n",
    "        best, best_label, last, loss = init_Kernel_train(data.y, K, options)\n",
    "        print(best, last)\n",
    "        best.extend(last)\n",
    "        final_accurancy[t, :] = np.array(best)\n",
    "        loss_value[t, :] = loss\n",
    "        np.savetxt(\n",
    "            'results/' + file_name[:-4] + \"_\" + str(options.eta) + \".npy\",\n",
    "            final_accurancy)\n",
    "        np.savetxt(\n",
    "            'results/' + file_name[:-4] + \"loss_value_\" + \"_\" +\n",
    "            str(options.eta) + \".npy\", loss_value)\n",
    "        np.savetxt(\n",
    "            'results/' + file_name[:-4] + \"_\" + \"alpha\" + str(options.alpha) +\n",
    "            \"beta\" + str(options.beta) + \"gamma\" + str(options.gamma) + \"eta\" +\n",
    "            str(options.eta) + \".npy\", best_label)\n",
    "    t += 1\n",
    "    torch.cuda.empty_cache()\n",
    "options.beta = 0\n",
    "for num in range(-2, 3, 1):\n",
    "    options.alpha = 2**num\n",
    "    for num_1 in range(-2, 3, 1):\n",
    "        options.gamma = 10**num_1\n",
    "        torch.cuda.empty_cache()\n",
    "        if np.sum(final_accurancy[t, :]) == 0:\n",
    "            with HiddenPrints():\n",
    "                best, best_label, last, loss = init_Kernel_train(\n",
    "                    data.y, K, options)\n",
    "            print(best, last)\n",
    "            best.extend(last)\n",
    "            final_accurancy[t, :] = np.array(best)\n",
    "            loss_value[t, :] = loss\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"_\" + str(options.eta) + \".npy\",\n",
    "                final_accurancy)\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"loss_value_\" + \"_\" +\n",
    "                str(options.eta) + \".npy\", loss_value)\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"_\" + \"alpha\" + str(options.alpha) +\n",
    "                \"beta\" + str(options.beta) + \"gamma\" + str(options.gamma) + \"eta\" +\n",
    "                str(options.eta) + \".npy\", best_label)\n",
    "        t += 1\n",
    "        torch.cuda.empty_cache()\n",
    "options.alpha = 0\n",
    "for num in range(-2, 3, 1):\n",
    "    options.beta = 2**num\n",
    "    for num_1 in range(-2, 3, 1):\n",
    "        options.gamma = 10**num_1\n",
    "        torch.cuda.empty_cache()\n",
    "        if np.sum(final_accurancy[t, :]) == 0:\n",
    "            with HiddenPrints():\n",
    "                best, best_label, last, loss = init_Kernel_train(\n",
    "                    data.y, K, options)\n",
    "            print(best, last)\n",
    "            best.extend(last)\n",
    "            final_accurancy[t, :] = np.array(best)\n",
    "            loss_value[t, :] = loss\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"_\" + str(options.eta) + \".npy\",\n",
    "                final_accurancy)\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"loss_value_\" + \"_\" +\n",
    "                str(options.eta) + \".npy\", loss_value)\n",
    "            np.savetxt(\n",
    "                'results/' + file_name[:-4] + \"_\" + \"alpha\" + str(options.alpha) +\n",
    "                \"beta\" + str(options.beta) + \"gamma\" + str(options.gamma) + \"eta\" +\n",
    "                str(options.eta) + \".npy\", best_label)\n",
    "        t += 1\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
